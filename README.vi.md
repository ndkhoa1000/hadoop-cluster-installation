üåê [English](./README.md) | Ti·∫øng Vi·ªát

# H∆∞·ªõng D·∫´n C√†i ƒê·∫∑t C·ª•m Apache Hadoop

H∆∞·ªõng d·∫´n to√†n di·ªán n√†y s·∫Ω h∆∞·ªõng d·∫´n b·∫°n c√†i ƒë·∫∑t v√† c·∫•u h√¨nh c·ª•m Apache Hadoop, bao g·ªìm c·∫£ thi·∫øt l·∫≠p single-node (pseudo-distributed) v√† multi-node (fully distributed).

## M·ª•c L·ª•c

1. [Y√™u C·∫ßu Ti√™n Quy·∫øt](#y√™u-c·∫ßu-ti√™n-quy·∫øt)
2. [T·∫£i Xu·ªëng v√† C√†i ƒê·∫∑t](#t·∫£i-xu·ªëng-v√†-c√†i-ƒë·∫∑t)
3. [C·∫•u H√¨nh M√¥i Tr∆∞·ªùng](#c·∫•u-h√¨nh-m√¥i-tr∆∞·ªùng)
4. [C·∫•u H√¨nh Hadoop](#c·∫•u-h√¨nh-hadoop)
5. [Thi·∫øt L·∫≠p Single-Node (Pseudo-Distributed)](#thi·∫øt-l·∫≠p-single-node)
6. [Thi·∫øt L·∫≠p Multi-Node (Fully Distributed)](#thi·∫øt-l·∫≠p-multi-node)
7. [Kh·ªüi ƒê·ªông C·ª•m](#kh·ªüi-ƒë·ªông-c·ª•m)
8. [X√°c Minh v√† Ki·ªÉm Tra](#x√°c-minh-v√†-ki·ªÉm-tra)
9. [C√°c V·∫•n ƒê·ªÅ Th∆∞·ªùng G·∫∑p v√† Kh·∫Øc Ph·ª•c S·ª± C·ªë](#kh·∫Øc-ph·ª•c-s·ª±-c·ªë)
10. [C√°c L·ªánh H·ªØu √çch](#c√°c-l·ªánh-h·ªØu-√≠ch)

## Y√™u C·∫ßu Ti√™n Quy·∫øt

### Y√™u C·∫ßu H·ªá Th·ªëng
- **H·ªá ƒêi·ªÅu H√†nh**: Linux, ho·∫∑c Windows (v·ªõi WSL)
- **Java**: OpenJDK 8, 11, ho·∫∑c 17 (khuy·∫øn ngh·ªã: OpenJDK 11)
- **B·ªô Nh·ªõ**: T·ªëi thi·ªÉu 4GB RAM (khuy·∫øn ngh·ªã 8GB+ cho multi-node)
- **Dung L∆∞·ª£ng ƒêƒ©a**: T·ªëi thi·ªÉu 20GB dung l∆∞·ª£ng kh·∫£ d·ª•ng
- **M·∫°ng**: Truy c·∫≠p SSH gi·ªØa c√°c node (cho thi·∫øt l·∫≠p multi-node)

### Ph·∫ßn M·ªÅm C·∫ßn Thi·∫øt
- Java Development Kit (JDK)
- SSH server v√† client
- rsync (ƒë·ªÉ ƒë·ªìng b·ªô h√≥a t·ªáp)

## T·∫£i Xu·ªëng v√† C√†i ƒê·∫∑t
### B∆∞·ªõc 0: Thi·∫øt L·∫≠p Ng∆∞·ªùi D√πng **master**
```bash
sudo adduser hadoop

# th√™m m·∫≠t kh·∫©u
sudo passwd hadoop

# th√™m hadoop v√†o nh√≥m sudo
sudo adduser hadoop sudo 

# chuy·ªÉn sang ng∆∞·ªùi d√πng hadoop
su hadoop

# ƒêi·ªÅu h∆∞·ªõng ƒë·∫øn th∆∞ m·ª•c home c·ªßa hadoop
cd ~
```

### B∆∞·ªõc 1: T·∫£i Xu·ªëng Hadoop

| Phi√™n B·∫£n | Ng√†y Ph√°t H√†nh | T·∫£i Xu·ªëng Binary |
|---------|--------------|-----------------|
| 3.4.1   | 18 Th√°ng 10, 2024  | [hadoop-3.4.1.tar.gz](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz) |

```bash
# T·∫£i xu·ªëng Hadoop 3.4.1
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz

# X√°c minh t·∫£i xu·ªëng (t√πy ch·ªçn)
wget https://downloads.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz.sha512

shasum -a 512 -c hadoop-3.4.1.tar.gz.sha512
```

### B∆∞·ªõc 2: Gi·∫£i N√©n v√† C√†i ƒê·∫∑t

```bash
# Gi·∫£i n√©n t·ªáp n√©n
tar -xzf hadoop-3.4.1.tar.gz

# Gi·ªØ n√≥ ·ªü v·ªã tr√≠ ∆∞a th√≠ch c·ªßa b·∫°n
mv hadoop-3.4.1 ~/hadoop
```

## C·∫•u H√¨nh M√¥i Tr∆∞·ªùng

### B∆∞·ªõc 1: C√†i ƒê·∫∑t Java (n·∫øu ch∆∞a c√†i ƒë·∫∑t)

#### Tr√™n Ubuntu/Debian:
```bash
sudo apt update && sudo apt install openjdk-11-jdk -y
```

#### Tr√™n CentOS/RHEL:
```bash
sudo yum install java-11-openjdk-devel
```

### B∆∞·ªõc 2: C·∫•u H√¨nh Bi·∫øn M√¥i Tr∆∞·ªùng

Th√™m nh·ªØng d√≤ng sau v√†o `~/.bashrc` ho·∫∑c `~/.profile` c·ªßa b·∫°n:

```bash
# M√¥i tr∆∞·ªùng Java
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64  # Linux

# M√¥i tr∆∞·ªùng Hadoop
export HADOOP_HOME=$HOME/hadoop  # ho·∫∑c ƒë∆∞·ªùng d·∫´n c√†i ƒë·∫∑t c·ªßa b·∫°n
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME

# Th√™m c√°c t·ªáp th·ª±c thi Hadoop v√†o PATH
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

√Åp d·ª•ng c√°c thay ƒë·ªïi:
```bash
source ~/.bashrc  # ho·∫∑c ~/.profile
```

### B∆∞·ªõc 3: C·∫•u H√¨nh SSH (C·∫ßn thi·∫øt cho c√°c ho·∫°t ƒë·ªông c·ª•m)

```bash
# T·∫°o c·∫∑p kh√≥a SSH (n·∫øu ch∆∞a t·ªìn t·∫°i)
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

# Th√™m kh√≥a c√¥ng khai v√†o authorized_keys
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# ƒê·∫∑t quy·ªÅn th√≠ch h·ª£p
chmod 0600 ~/.ssh/authorized_keys

# Ki·ªÉm tra SSH ƒë·∫øn localhost
ssh localhost
```

## C·∫•u H√¨nh Hadoop

ƒêi·ªÅu h∆∞·ªõng ƒë·∫øn th∆∞ m·ª•c c·∫•u h√¨nh Hadoop:
```bash
cd $HADOOP_HOME/etc/hadoop
```

### B∆∞·ªõc 1: C·∫•u H√¨nh `hadoop-env.sh`

```bash
# Ch·ªânh s·ª≠a hadoop-env.sh
vim hadoop-env.sh

# Th√™m ho·∫∑c c·∫≠p nh·∫≠t JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

### B∆∞·ªõc 2: C·∫•u H√¨nh C√°c Th√†nh Ph·∫ßn C·ªët L√µi

#### `core-site.xml`
```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
        <description>URI h·ªá th·ªëng t·ªáp m·∫∑c ƒë·ªãnh</description>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>$HADOOP_HOME/tmp</value>
        <description>Th∆∞ m·ª•c t·∫°m th·ªùi cho Hadoop</description>
    </property>
</configuration>
```

#### `hdfs-site.xml`
```xml
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>$HADOOP_HOME/data/namenode</value>
        <description>Th∆∞ m·ª•c cho metadata namenode</description>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>$HADOOP_HOME/data/datanode</value>
        <description>Th∆∞ m·ª•c cho d·ªØ li·ªáu datanode</description>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
        <description>Sao ch√©p kh·ªëi m·∫∑c ƒë·ªãnh</description>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>$HADOOP_HOME/data/secondary</value>
        <description>Th∆∞ m·ª•c checkpoint secondary namenode</description>
    </property>
</configuration>
```

#### `mapred-site.xml`
```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
        <description>T√™n framework MapReduce</description>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>
```

#### `yarn-site.xml`
```xml
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>localhost</value>
        <description>Hostname ResourceManager</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>D·ªãch v·ª• ph·ª• tr·ª£ cho NodeManager</description>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
```

### B∆∞·ªõc 3: T·∫°o C√°c Th∆∞ M·ª•c C·∫ßn Thi·∫øt

```bash
# T·∫°o th∆∞ m·ª•c d·ªØ li·ªáu
sudo mkdir -p $HADOOP_HOME/data/{namenode,datanode,secondary}
sudo mkdir -p $HADOOP_HOME/tmp

# ƒê·∫∑t quy·ªÅn s·ªü h·ªØu th√≠ch h·ª£p
sudo chown -R $USER:$USER $HADOOP_HOME/data
sudo chown -R $USER:$USER $HADOOP_HOME/tmp
```

## Thi·∫øt L·∫≠p Single-Node

### ƒê·ªãnh D·∫°ng NameNode (Ch·ªâ thi·∫øt l·∫≠p l·∫ßn ƒë·∫ßu)

```bash
hdfs namenode -format
```

### Kh·ªüi ƒê·ªông D·ªãch V·ª• Hadoop

```bash
# Kh·ªüi ƒë·ªông HDFS
start-dfs.sh

# Kh·ªüi ƒë·ªông YARN
start-yarn.sh

# Ho·∫∑c kh·ªüi ƒë·ªông t·∫•t c·∫£ d·ªãch v·ª• c√πng l√∫c
start-all.sh
```

### X√°c Minh C√†i ƒê·∫∑t

```bash
# Ki·ªÉm tra c√°c ti·∫øn tr√¨nh ƒëang ch·∫°y
jps

# ƒê·∫ßu ra d·ª± ki·∫øn n√™n bao g·ªìm:
# - NameNode
# - DataNode
# - ResourceManager
# - NodeManager
# - SecondaryNameNode
```

## Thi·∫øt L·∫≠p Multi-Node

### Y√™u C·∫ßu Ti√™n Quy·∫øt cho Multi-Node

1. Nhi·ªÅu m√°y c√≥ c√†i ƒë·∫∑t Hadoop
2. K·∫øt n·ªëi m·∫°ng gi·ªØa t·∫•t c·∫£ c√°c node
3. Truy c·∫≠p SSH t·ª´ master ƒë·∫øn t·∫•t c·∫£ c√°c node slave
4. C√πng t√™n ng∆∞·ªùi d√πng tr√™n t·∫•t c·∫£ c√°c node
5. Th·ªùi gian ƒë·ªìng b·ªô tr√™n t·∫•t c·∫£ c√°c node

### B∆∞·ªõc 1: C·∫•u H√¨nh Master Node

#### C·∫≠p nh·∫≠t `core-site.xml` tr√™n master:
```xml
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
</property>
```

#### C·∫≠p nh·∫≠t `yarn-site.xml` tr√™n master:
```xml
<property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
</property>
```

#### C·∫•u h√¨nh t·ªáp workers:
```bash
# Ch·ªânh s·ª≠a $HADOOP_HOME/etc/hadoop/workers
vim $HADOOP_HOME/etc/hadoop/workers

# Th√™m t√™n host c·ªßa c√°c worker node (m·ªôt node m·ªói d√≤ng)
# X√≥a localhost cho c·ª•m (n·∫øu c√≥)
node1
node2
node3
```

### B∆∞·ªõc 1.5: C·∫•u H√¨nh Ph√¢n B·ªï B·ªô Nh·ªõ
Ph√¢n b·ªï b·ªô nh·ªõ c√≥ th·ªÉ kh√≥ khƒÉn tr√™n c√°c node RAM th·∫•p v√¨ c√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh kh√¥ng ph√π h·ª£p v·ªõi c√°c node c√≥ √≠t h∆°n 8GB RAM. Ph·∫ßn n√†y s·∫Ω l√†m n·ªïi b·∫≠t c√°ch ph√¢n b·ªï b·ªô nh·ªõ ho·∫°t ƒë·ªông cho c√°c c√¥ng vi·ªác MapReduce v√† cung c·∫•p c·∫•u h√¨nh m·∫´u cho c√°c node **2GB RAM**.

1. C·∫≠p nh·∫≠t `yarn-site.xml`:
```xml
<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
</property>

<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>
```
2. C·∫≠p nh·∫≠t `mapred-site.xml`:
```xml
<property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <value>512</value>
</property>

<property>
        <name>mapreduce.map.memory.mb</name>
        <value>256</value>
</property>
<property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>256</value>
</property>
```

### B∆∞·ªõc 2: C·∫•u H√¨nh Slave Node

1. Sao ch√©p to√†n b·ªô c·∫•u h√¨nh Hadoop t·ª´ master ƒë·∫øn t·∫•t c·∫£ c√°c slave node:
```bash
scp -r $HADOOP_HOME user@worker-node:~/
```

2. C·∫≠p nh·∫≠t `hdfs-site.xml` tr√™n slaves ƒë·ªÉ tr·ªè ƒë·∫øn master:
```xml
<property>
    <name>dfs.namenode.name.dir</name>
    <value>$HADOOP_HOME/data/namenode</value>
</property>
```

### B∆∞·ªõc 3: C·∫•u H√¨nh M·∫°ng

C·∫≠p nh·∫≠t `/etc/hosts` tr√™n **t·∫•t c·∫£ c√°c node**:
```bash
# Th√™m c√°c m·ª•c cho t·∫•t c·∫£ c√°c node
192.168.1.100   master
192.168.1.101   node1
192.168.1.102   node2
192.168.1.103   node3
```

### B∆∞·ªõc 4: Kh·ªüi ƒê·ªông C·ª•m Multi-Node

T·ª´ master node:
```bash
# ƒê·ªãnh d·∫°ng namenode (ch·ªâ l·∫ßn ƒë·∫ßu ti√™n)
hdfs namenode -format

# Kh·ªüi ƒë·ªông c·ª•m
start-all.sh
```

## Kh·ªüi ƒê·ªông v√† D·ª´ng C·ª•m Hadoop

### üéØ **Kh·ªüi ƒê·ªông Nhanh (Khuy·∫øn ngh·ªã, s·ª≠ d·ª•ng script ƒë∆∞·ª£c vi·∫øt trong repository Github n√†y)**

S·ª≠ d·ª•ng script ƒëi·ªÅu khi·ªÉn ti·ªán l·ª£i ƒë∆∞·ª£c bao g·ªìm trong repository n√†y:

```bash
# Kh·ªüi ƒë·ªông c·ª•m
./hadoop-control.sh start

# D·ª´ng c·ª•m  
./hadoop-control.sh stop

# Kh·ªüi ƒë·ªông l·∫°i c·ª•m
./hadoop-control.sh restart

# Ki·ªÉm tra tr·∫°ng th√°i c·ª•m
./hadoop-control.sh status

# Hi·ªÉn th·ªã tr·ª£ gi√∫p
./hadoop-control.sh help
```

Script n√†y cung c·∫•p:
- ‚úÖ **ƒê·∫ßu ra c√≥ m√†u** ƒë·ªÉ d·ªÖ ƒë·ªçc
- ‚úÖ **Ki·ªÉm tra l·ªói t·ª± ƒë·ªông** v√† x√°c th·ª±c
- ‚úÖ **Ph√°t hi·ªán d·ªãch v·ª• th√¥ng minh** - bi·∫øt nh·ªØng g√¨ ƒëang ch·∫°y
- ‚úÖ **Li√™n k·∫øt giao di·ªán web** khi c·ª•m ho·∫°t ƒë·ªông t·ªët
- ‚úÖ **Quy tr√¨nh kh·ªüi ƒë·ªông/d·ª´ng an to√†n** v·ªõi tr√¨nh t·ª± th√≠ch h·ª£p

### üöÄ Qu·∫£n L√Ω D·ªãch V·ª• Th·ªß C√¥ng

#### Ph∆∞∆°ng ph√°p 1: Kh·ªüi ƒê·ªông T·∫•t C·∫£ D·ªãch V·ª• C√πng L√∫c

```bash
# Kh·ªüi ƒë·ªông t·∫•t c·∫£ d·ªãch v·ª• Hadoop
start-all.sh

# Ki·ªÉm tra xem t·∫•t c·∫£ d·ªãch v·ª• c√≥ ƒëang ch·∫°y kh√¥ng
jps
```

#### Ph∆∞∆°ng ph√°p 2: Kh·ªüi ƒê·ªông D·ªãch V·ª• Ri√™ng L·∫ª

```bash
# 1. Kh·ªüi ƒë·ªông d·ªãch v·ª• HDFS (NameNode, DataNode, SecondaryNameNode)
start-dfs.sh

# 2. Kh·ªüi ƒë·ªông d·ªãch v·ª• YARN (ResourceManager, NodeManager)
start-yarn.sh

# 3. Kh·ªüi ƒë·ªông MapReduce Job History Server (t√πy ch·ªçn)
mapred --daemon start historyserver
```

#### Ph∆∞∆°ng ph√°p 3: Kh·ªüi ƒê·ªông D·ªãch V·ª• T·ª´ng C√°i M·ªôt

```bash
# Kh·ªüi ƒë·ªông NameNode
hdfs --daemon start namenode

# Kh·ªüi ƒë·ªông DataNode
hdfs --daemon start datanode

# Kh·ªüi ƒë·ªông SecondaryNameNode
hdfs --daemon start secondarynamenode

# Kh·ªüi ƒë·ªông ResourceManager
yarn --daemon start resourcemanager

# Kh·ªüi ƒë·ªông NodeManager
yarn --daemon start nodemanager

# Kh·ªüi ƒë·ªông Job History Server
mapred --daemon start historyserver
```

### ‚èπÔ∏è D·ª´ng D·ªãch V·ª• Hadoop

#### D·ª´ng T·∫•t C·∫£ D·ªãch V·ª•

```bash
# D·ª´ng t·∫•t c·∫£ d·ªãch v·ª• Hadoop
stop-all.sh
```

#### D·ª´ng D·ªãch V·ª• Ri√™ng L·∫ª

```bash
# D·ª´ng d·ªãch v·ª• YARN
stop-yarn.sh

# D·ª´ng d·ªãch v·ª• HDFS
stop-dfs.sh

# D·ª´ng Job History Server
mapred --daemon stop historyserver
```

#### D·ª´ng D·ªãch V·ª• T·ª´ng C√°i M·ªôt

```bash
# D·ª´ng Job History Server
mapred --daemon stop historyserver

# D·ª´ng NodeManager
yarn --daemon stop nodemanager

# D·ª´ng ResourceManager
yarn --daemon stop resourcemanager

# D·ª´ng SecondaryNameNode
hdfs --daemon stop secondarynamenode

# D·ª´ng DataNode
hdfs --daemon stop datanode

# D·ª´ng NameNode
hdfs --daemon stop namenode
```

### üîÑ Kh·ªüi ƒê·ªông L·∫°i D·ªãch V·ª•

```bash
# Kh·ªüi ƒë·ªông l·∫°i t·∫•t c·∫£ d·ªãch v·ª•
stop-all.sh && start-all.sh

# Kh·ªüi ƒë·ªông l·∫°i ch·ªâ HDFS
stop-dfs.sh && start-dfs.sh

# Kh·ªüi ƒë·ªông l·∫°i ch·ªâ YARN
stop-yarn.sh && start-yarn.sh
```

### ‚úÖ X√°c Minh D·ªãch V·ª• ƒêang Ch·∫°y

#### Ki·ªÉm Tra C√°c Ti·∫øn Tr√¨nh Java ƒêang Ch·∫°y

```bash
# Li·ªát k√™ t·∫•t c·∫£ c√°c ti·∫øn tr√¨nh Java li√™n quan ƒë·∫øn Hadoop
jps

# ƒê·∫ßu ra d·ª± ki·∫øn n√™n bao g·ªìm:
# 12345 NameNode
# 12346 DataNode
# 12347 SecondaryNameNode
# 12348 ResourceManager
# 12349 NodeManager
# 12350 JobHistoryServer (n·∫øu ƒë∆∞·ª£c kh·ªüi ƒë·ªông)
```

#### Ki·ªÉm Tra Tr·∫°ng Th√°i D·ªãch V·ª• C·ª• Th·ªÉ

```bash
# Ki·ªÉm tra xem NameNode c√≥ ƒëang ch·∫°y kh√¥ng
hdfs dfsadmin -report

# Ki·ªÉm tra xem YARN c√≥ ƒëang ch·∫°y kh√¥ng
yarn node -list

# Ki·ªÉm tra s·ª©c kh·ªèe c·ª•m
hdfs dfsadmin -safemode get
```

### üß™ Ki·ªÉm Tra C√†i ƒê·∫∑t Hadoop

#### Test 1: C√°c Thao T√°c HDFS C∆° B·∫£n

```bash
# T·∫°o th∆∞ m·ª•c ng∆∞·ªùi d√πng c·ªßa b·∫°n trong HDFS
hdfs dfs -mkdir -p /user/$USER

# T·∫°o th∆∞ m·ª•c ki·ªÉm tra
hdfs dfs -mkdir /user/$USER/test

# Li·ªát k√™ th∆∞ m·ª•c g·ªëc HDFS
hdfs dfs -ls /

# Li·ªát k√™ th∆∞ m·ª•c ng∆∞·ªùi d√πng c·ªßa b·∫°n
hdfs dfs -ls /user/$USER

# Ki·ªÉm tra s·ª©c kh·ªèe HDFS
hdfs fsck /
```

#### Test 2: T·∫£i L√™n v√† T·∫£i Xu·ªëng T·ªáp

```bash
# T·∫°o t·ªáp ki·ªÉm tra c·ª•c b·ªô
echo "Xin ch√†o Th·∫ø gi·ªõi Hadoop!" > test.txt
echo "ƒê√¢y l√† t·ªáp ki·ªÉm tra cho Hadoop HDFS" >> test.txt

# T·∫£i t·ªáp l√™n HDFS
hdfs dfs -put test.txt /user/$USER/

# Li·ªát k√™ t·ªáp trong HDFS
hdfs dfs -ls /user/$USER/

# Xem n·ªôi dung t·ªáp trong HDFS
hdfs dfs -cat /user/$USER/test.txt

# T·∫£i xu·ªëng t·ªáp t·ª´ HDFS
hdfs dfs -get /user/$USER/test.txt downloaded_test.txt

# X√°c minh t·ªáp ƒë√£ t·∫£i xu·ªëng
cat downloaded_test.txt

# D·ªçn d·∫πp t·ªáp ki·ªÉm tra
rm test.txt downloaded_test.txt
hdfs dfs -rm /user/$USER/test.txt
```

#### Test 3: Ch·∫°y C√¥ng Vi·ªác MapReduce M·∫´u

```bash
# T·∫°o th∆∞ m·ª•c ƒë·∫ßu v√†o cho MapReduce
hdfs dfs -mkdir /input

# Sao ch√©p t·ªáp c·∫•u h√¨nh Hadoop l√†m ƒë·∫ßu v√†o
hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml /input/

# Ch·∫°y v√≠ d·ª• ƒë·∫øm t·ª´
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /input /output

# Ki·ªÉm tra ƒë·∫ßu ra
hdfs dfs -ls /output/
hdfs dfs -cat /output/part-r-00000 | head -20

# D·ªçn d·∫πp
hdfs dfs -rm -r /output
hdfs dfs -rm -r /input
```

#### Test 4: Ki·ªÉm Tra ·ª®ng D·ª•ng YARN

```bash
# Ch·∫°y ·ª©ng d·ª•ng YARN ƒë∆°n gi·∫£n (t√≠nh to√°n Pi)
yarn jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi 2 100

# Ki·ªÉm tra l·ªãch s·ª≠ ·ª©ng d·ª•ng
yarn application -list -appStates ALL
```

#### Test 5: Ki·ªÉm Tra Benchmark Hi·ªáu Su·∫•t

```bash
# Test Ghi TestDFSIO
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -write -nrFiles 4 -fileSize 128MB

# Test ƒê·ªçc TestDFSIO
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -read -nrFiles 4 -fileSize 128MB

# D·ªçn d·∫πp d·ªØ li·ªáu ki·ªÉm tra
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -clean
```

### üìä Gi√°m S√°t v√† Ki·ªÉm Tra S·ª©c Kh·ªèe

#### Truy C·∫≠p Giao Di·ªán Web

```bash
# M·ªü giao di·ªán web (ch·∫°y c√°c l·ªánh n√†y ƒë·ªÉ l·∫•y URL)
echo "NameNode Web UI: http://localhost:9870"
echo "ResourceManager Web UI: http://localhost:8088"
echo "Job History Server: http://localhost:19888"
echo "DataNode Web UI: http://localhost:9864"
echo "NodeManager Web UI: http://localhost:8042"
```

#### Gi√°m S√°t D√≤ng L·ªánh

```bash
# Ki·ªÉm tra t√≥m t·∫Øt c·ª•m
hdfs dfsadmin -report

# Ki·ªÉm tra h·ªá th·ªëng t·ªáp
hdfs fsck /

# Gi√°m s√°t ·ª©ng d·ª•ng YARN
yarn top

# Ki·ªÉm tra tr·∫°ng th√°i node
yarn node -list -showDetails

# Xem metrics c·ª•m
yarn cluster -lnl
```

## üîß Qu·∫£n L√Ω Node C·ª•m

### Script Qu·∫£n L√Ω Node

T√¥i ƒë√£ t·∫°o script to√†n di·ªán ƒë·ªÉ qu·∫£n l√Ω c√°c node c·ª•m:

```bash
# L√†m cho script c√≥ th·ªÉ th·ª±c thi (n·∫øu ch∆∞a)
chmod +x ./manage-cluster-nodes.sh

# Hi·ªÉn th·ªã t·∫•t c·∫£ c√°c l·ªánh c√≥ s·∫µn
./manage-cluster-nodes.sh help
```

### C√°c T√≠nh NƒÉng Ch√≠nh

#### üìã **Th√¥ng Tin Node**
```bash
# Li·ªát k√™ t·∫•t c·∫£ c√°c node hi·ªán t·∫°i
./manage-cluster-nodes.sh list

# Hi·ªÉn th·ªã tr·∫°ng th√°i c·ª•m chi ti·∫øt
./manage-cluster-nodes.sh status
```

#### ‚ûï **Th√™m Node**
```bash
# Th√™m worker node m·ªõi
./manage-cluster-nodes.sh add node1

# Th√™m node theo ƒë·ªãa ch·ªâ IP
./manage-cluster-nodes.sh add 192.168.1.100
```

#### ‚ûñ **X√≥a Node (Quy Tr√¨nh An To√†n)**
```bash
# B∆∞·ªõc 1: Ng·ª´ng ho·∫°t ƒë·ªông node m·ªôt c√°ch an to√†n
./manage-cluster-nodes.sh decommission node1

# B∆∞·ªõc 2: X√≥a kh·ªèi c·ª•m (sau khi ng·ª´ng ho·∫°t ƒë·ªông ho√†n th√†nh)
./manage-cluster-nodes.sh remove node1
```

#### üîÑ **Qu·∫£n L√Ω Node**
```bash
# ƒê∆∞a l·∫°i node ƒë√£ ng·ª´ng ho·∫°t ƒë·ªông
./manage-cluster-nodes.sh recommission node1
```

#### üèóÔ∏è **Chuy·ªÉn ƒê·ªïi C·ª•m**
```bash
# Chuy·ªÉn ƒë·ªïi thi·∫øt l·∫≠p single-node sang multi-node
./manage-cluster-nodes.sh convert-multi

# Chuy·ªÉn ƒë·ªïi multi-node tr·ªü l·∫°i single-node
./manage-cluster-nodes.sh convert-single
```

#### üíæ **Sao L∆∞u v√† Kh√¥i Ph·ª•c C·∫•u H√¨nh**
```bash
# Sao l∆∞u c·∫•u h√¨nh hi·ªán t·∫°i
./manage-cluster-nodes.sh backup

# Kh√¥i ph·ª•c t·ª´ sao l∆∞u
./manage-cluster-nodes.sh restore
```

### Th√™m Worker Node M·ªõi - Quy Tr√¨nh Ho√†n Ch·ªânh

1. **Th√™m node v√†o c·∫•u h√¨nh c·ª•m:**
   ```bash
   ./manage-cluster-nodes.sh add node2
   ```

2. **Thi·∫øt l·∫≠p truy c·∫≠p SSH kh√¥ng c·∫ßn m·∫≠t kh·∫©u:**
   ```bash
   # Sao ch√©p kh√≥a SSH ƒë·∫øn node m·ªõi
   ssh-copy-id user@node2
   
   # Ki·ªÉm tra truy c·∫≠p SSH
   ssh node2
   ```

3. **C√†i ƒë·∫∑t Hadoop tr√™n node m·ªõi:**
   ```bash
   # Sao ch√©p c√†i ƒë·∫∑t Hadoop ƒë·∫øn node m·ªõi
   scp -r $HADOOP_HOME user@node2:~/
   ```

4. **Sao ch√©p t·ªáp c·∫•u h√¨nh:**
   ```bash
   # Sao ch√©p c·∫•u h√¨nh ƒë·∫øn node m·ªõi
   scp -r $HADOOP_HOME/etc/hadoop/* user@node2:$HADOOP_HOME/etc/hadoop/
   ```

5. **L√†m m·ªõi c√°c node c·ª•m:**
   ```bash
   # L√†m m·ªõi node YARN
   yarn rmadmin -refreshNodes
   
   # L√†m m·ªõi node HDFS
   hdfs dfsadmin -refreshNodes
   ```

6. **Kh·ªüi ƒë·ªông d·ªãch v·ª• tr√™n node m·ªõi:**
   ```bash
   # Tr√™n worker node m·ªõi, kh·ªüi ƒë·ªông DataNode v√† NodeManager
   ssh node2 "$HADOOP_HOME/bin/hdfs --daemon start datanode"
   ssh node2 "$HADOOP_HOME/bin/yarn --daemon start nodemanager"
   ```

### X√≥a Node M·ªôt C√°ch An To√†n - Quy Tr√¨nh Ho√†n Ch·ªânh

1. **Ng·ª´ng ho·∫°t ƒë·ªông node:**
   ```bash
   ./manage-cluster-nodes.sh decommission node2
   ```

2. **Gi√°m s√°t ti·∫øn tr√¨nh ng·ª´ng ho·∫°t ƒë·ªông:**
   ```bash
   # Ki·ªÉm tra tr·∫°ng th√°i ng·ª´ng ho·∫°t ƒë·ªông HDFS
   hdfs dfsadmin -report
   
   # Ki·ªÉm tra tr·∫°ng th√°i node YARN
   yarn node -list -all
   ```

3. **Ch·ªù ng·ª´ng ho·∫°t ƒë·ªông ho√†n th√†nh** (c√°c kh·ªëi d·ªØ li·ªáu ƒë∆∞·ª£c chuy·ªÉn ƒë·∫øn c√°c node kh√°c)

4. **X√≥a node:**
   ```bash
   ./manage-cluster-nodes.sh remove node2
   ```

5. **D·ª´ng d·ªãch v·ª• tr√™n node ƒë√£ x√≥a:**
   ```bash
   ssh node2 "$HADOOP_HOME/bin/yarn --daemon stop nodemanager"
   ssh node2 "$HADOOP_HOME/bin/hdfs --daemon stop datanode"
   ```

### T·ªáp C·∫•u H√¨nh ƒê∆∞·ª£c S·ª≠a ƒê·ªïi

Script t·ª± ƒë·ªông qu·∫£n l√Ω c√°c t·ªáp n√†y:
- `$HADOOP_HOME/etc/hadoop/workers` - Danh s√°ch c√°c worker node
- `$HADOOP_HOME/etc/hadoop/core-site.xml` - C·∫•u h√¨nh c·ªët l√µi
- `$HADOOP_HOME/etc/hadoop/yarn-site.xml` - C·∫•u h√¨nh YARN  
- `$HADOOP_HOME/etc/hadoop/dfs.exclude` - Danh s√°ch ng·ª´ng ho·∫°t ƒë·ªông HDFS
- `$HADOOP_HOME/etc/hadoop/yarn.exclude` - Danh s√°ch ng·ª´ng ho·∫°t ƒë·ªông YARN

### Sao L∆∞u v√† Kh√¥i Ph·ª•c

T·∫•t c·∫£ c√°c thay ƒë·ªïi c·∫•u h√¨nh ƒë∆∞·ª£c t·ª± ƒë·ªông sao l∆∞u v√†o:
```
$HADOOP_HOME/backups/hadoop_config_YYYYMMDD_HHMMSS.tar.gz
```

B·∫°n c√≥ th·ªÉ kh√¥i ph·ª•c b·∫•t k·ª≥ b·∫£n sao l∆∞u n√†o b·∫±ng c√°ch s·ª≠ d·ª•ng:
```bash
./manage-cluster-nodes.sh restore
```

### Tham Kh·∫£o Nhanh

ƒê·ªÉ c√≥ h∆∞·ªõng d·∫´n tham kh·∫£o nhanh ho√†n ch·ªânh, xem: `CLUSTER-MANAGEMENT.md`

---
## X√°c Minh v√† Ki·ªÉm Tra

### Ki·ªÉm Tra Tr·∫°ng Th√°i C·ª•m

```bash
# Ki·ªÉm tra tr·∫°ng th√°i HDFS
hdfs dfsadmin -report

# Ki·ªÉm tra node YARN
yarn node -list

# Ki·ªÉm tra c√°c ti·∫øn tr√¨nh ƒëang ch·∫°y
jps
```

### Giao Di·ªán Web

- **HDFS NameNode**: http://localhost:9870
- **YARN ResourceManager**: http://localhost:8088
- **MapReduce Job History**: http://localhost:19888

### C√°c Thao T√°c HDFS C∆° B·∫£n

```bash
# T·∫°o th∆∞ m·ª•c trong HDFS
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/$USER

# Li·ªát k√™ n·ªôi dung HDFS
hdfs dfs -ls /

# Sao ch√©p t·ªáp ƒë·∫øn HDFS
hdfs dfs -put /path/to/local/file /user/$USER/

# Sao ch√©p t·ªáp t·ª´ HDFS
hdfs dfs -get /user/$USER/file /path/to/local/
```

### Ch·∫°y C√¥ng Vi·ªác MapReduce M·∫´u

```bash
# T·∫°o th∆∞ m·ª•c ƒë·∫ßu v√†o
hdfs dfs -mkdir /input

# Sao ch√©p t·ªáp ƒë·∫ßu v√†o
hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml /input

# Ch·∫°y v√≠ d·ª• ƒë·∫øm t·ª´
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar wordcount /input /output

# Ki·ªÉm tra ƒë·∫ßu ra
hdfs dfs -cat /output/part-r-00000
```

## Kh·∫Øc Ph·ª•c S·ª± C·ªë

### C√°c V·∫•n ƒê·ªÅ Th∆∞·ªùng G·∫∑p v√† Gi·∫£i Ph√°p

#### 0. ƒê·ªìng b·ªô t·ªáp nhanh s·ª≠ d·ª•ng **rsync**:
S·ª≠ d·ª•ng ***scp*** c√≥ th·ªÉ kh√¥ng ƒë·ªìng b·ªô t·ªáp tr√™n t·∫•t c·∫£ c√°c node. V·∫≠y ***rsync*** s·∫Ω √°nh x·∫° v√† thay th·∫ø t·∫•t c·∫£ c√°c t·ªáp c√≥ c√πng t√™n v√† ƒë·∫£m b·∫£o ch√∫ng s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t ch√≠nh x√°c.
```bash
rsync -avz ~/hadoop/etc/hadoop/ user@node1:~/hadoop/etc/hadoop/
```

#### 1. L·ªói Li√™n Quan ƒê·∫øn Java
```bash
# X√°c minh c√†i ƒë·∫∑t Java
java -version
echo $JAVA_HOME

# ƒê·∫£m b·∫£o JAVA_HOME ƒë∆∞·ª£c thi·∫øt l·∫≠p trong hadoop-env.sh
```

#### 2. V·∫•n ƒê·ªÅ K·∫øt N·ªëi SSH
```bash
# Ki·ªÉm tra k·∫øt n·ªëi SSH
ssh localhost
ssh node1

# Ki·ªÉm tra thi·∫øt l·∫≠p kh√≥a SSH
ls -la ~/.ssh/
```

#### 3. L·ªói T·ª´ Ch·ªëi Quy·ªÅn
```bash
# S·ª≠a quy·ªÅn th∆∞ m·ª•c
sudo chown -R $USER:$USER $HADOOP_HOME
chmod 755 $HADOOP_HOME/data/*
```

#### 4. C·ªïng ƒê√£ ƒê∆∞·ª£c S·ª≠ D·ª•ng
```bash
# Ki·ªÉm tra xem c√°i g√¨ ƒëang s·ª≠ d·ª•ng c·ªïng
netstat -tulpn | grep :9000
lsof -i :9000

# K·∫øt th√∫c ti·∫øn tr√¨nh n·∫øu c·∫ßn thi·∫øt
kill -9 <PID>
```

#### 5. DataNode Kh√¥ng Kh·ªüi ƒê·ªông
```bash
# Ki·ªÉm tra logs
tail -f $HADOOP_HOME/logs/hadoop-*-datanode-*.log

# Gi·∫£i ph√°p ph·ªï bi·∫øn: X√≥a v√† ƒë·ªãnh d·∫°ng l·∫°i
stop-all.sh
rm -rf $HADOOP_HOME/data/datanode/*
hdfs namenode -format -force
start-all.sh
```

### V·ªã Tr√≠ T·ªáp Log

```bash
# Th∆∞ m·ª•c logs Hadoop
$HADOOP_HOME/logs/

# C√°c t·ªáp log quan tr·ªçng
hadoop-*-namenode-*.log
hadoop-*-datanode-*.log
yarn-*-resourcemanager-*.log
yarn-*-nodemanager-*.log
```

## C√°c L·ªánh H·ªØu √çch

### L·ªánh HDFS
```bash
# Ki·ªÉm tra h·ªá th·ªëng t·ªáp
hdfs fsck /

# C√°c thao t√°c safe mode
hdfs dfsadmin -safemode leave
hdfs dfsadmin -safemode enter

# C√¢n b·∫±ng c·ª•m
hdfs balancer

# Ng·ª´ng ho·∫°t ƒë·ªông node
hdfs dfsadmin -refreshNodes
```

### L·ªánh YARN
```bash
# Li·ªát k√™ ·ª©ng d·ª•ng
yarn application -list

# K·∫øt th√∫c ·ª©ng d·ª•ng
yarn application -kill <application_id>

# Qu·∫£n l√Ω node
yarn node -list -all
yarn rmadmin -refreshNodes
```

### Qu·∫£n Tr·ªã C·ª•m
```bash
# Ki·ªÉm tra s·ª©c kh·ªèe c·ª•m
hdfs dfsadmin -report
yarn node -list -showDetails

# Gi√°m s√°t c·ª•m
hadoop dfsadmin -printTopology
yarn top
```

### Gi√°m S√°t Hi·ªáu Su·∫•t
```bash
# Ki·ªÉm tra s·ª≠ d·ª•ng ƒëƒ©a
hdfs dfs -du -h /

# Gi√°m s√°t t√†i nguy√™n h·ªá th·ªëng
top
htop
iostat -x 1
```

## T√†i Li·ªáu Tham Kh·∫£o

- [T√†i Li·ªáu Hadoop Ch√≠nh Th·ª©c](https://hadoop.apache.org/docs/current/)
- [H∆∞·ªõng D·∫´n Thi·∫øt L·∫≠p C·ª•m](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html)
- [Ki·∫øn Tr√∫c HDFS](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)
- [Ki·∫øn Tr√∫c YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)

## H·ªó Tr·ª£

ƒê·ªëi v·ªõi c√°c v·∫•n ƒë·ªÅ v√† c√¢u h·ªèi:
- Ki·ªÉm tra [H∆∞·ªõng D·∫´n Kh·∫Øc Ph·ª•c S·ª± C·ªë Hadoop](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Troubleshooting.html)
- Gh√© thƒÉm [Danh S√°ch G·ª≠i Th∆∞ Ng∆∞·ªùi D√πng Apache Hadoop](https://hadoop.apache.org/mailing_lists.html)
- G·ª≠i v·∫•n ƒë·ªÅ ƒë·∫øn [Apache Hadoop JIRA](https://issues.apache.org/jira/projects/HADOOP)

---

**L∆∞u √Ω**: H∆∞·ªõng d·∫´n n√†y d·ª±a tr√™n Hadoop 3.4.1. C·∫•u h√¨nh c√≥ th·ªÉ thay ƒë·ªïi m·ªôt ch√∫t cho c√°c phi√™n b·∫£n kh√°c nhau.